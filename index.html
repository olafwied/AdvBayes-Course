<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Advanced Bayesian ML - Olaf Wied</title>

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
		
		<!-- Load mathjax -->
	    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
	    <!-- MathJax configuration -->
	    <script type="text/x-mathjax-config">
	    MathJax.Hub.Config({
		tex2jax: {
		    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
		    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
		    processEscapes: true,
		    processEnvironments: true
		},
		// Center justify equations in code and markdown cells. Elsewhere
		// we use CSS to left justify single line equations in code cells.
		displayAlign: 'center',
		"HTML-CSS": {
		    styles: {'.MathJax_Display': {"margin": 0}},
		    linebreaks: { automatic: true }
		}
	    });
	    </script>
	    <!-- End of mathjax configuration -->
		
		
	<!-- Get Font-awesome from cdn -->
	<link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.css">
	<link rel="stylesheet" href="css/custom.css">	
	</head>
	<body>
		<div class="reveal">
<div class="slides">
<section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Scalable-Methods-for-Bayesian-Machine-Learning-and-Probabilistic-Inference">Scalable Methods for Bayesian Machine Learning and Probabilistic Inference<a class="anchor-link" href="#Scalable-Methods-for-Bayesian-Machine-Learning-and-Probabilistic-Inference">&#182;</a></h1><h2 id="And-other-advanced-topics">And other advanced topics<a class="anchor-link" href="#And-other-advanced-topics">&#182;</a></h2><h2 id="&#169;-Olaf-Wied-2018">&#169; Olaf Wied 2018<a class="anchor-link" href="#&#169;-Olaf-Wied-2018">&#182;</a></h2><p>Note that this is intended for an advanced audience. Background in machine learning and deep learning, linear algebra and probability theory is recommended.</p>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<h2 id="Scalable-Methods-for-Bayesian-Machine-Learning-and-Probabilistic-Inference">Scalable Methods for Bayesian Machine Learning and Probabilistic Inference<a class="anchor-link" href="#Scalable-Methods-for-Bayesian-Machine-Learning-and-Probabilistic-Inference">&#182;</a></h2><h1 id="Outline">Outline<a class="anchor-link" href="#Outline">&#182;</a></h1><h3 id="Part-I---Short-Recap-of-Bayesian-Basics">Part I - Short Recap of Bayesian Basics<a class="anchor-link" href="#Part-I---Short-Recap-of-Bayesian-Basics">&#182;</a></h3><ul>
<li>Bayes Theorem</li>
<li>Bayesian Inference</li>
<li>Generative vs Discriminative Models</li>
<li>Bayesian Statistics</li>
<li>Priors</li>
<li>Frequentist Statistics</li>
</ul>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<h3 id="Part-II---Latent-Variable-Models-and-the-EM-Algorithm">Part II - Latent Variable Models and the EM Algorithm<a class="anchor-link" href="#Part-II---Latent-Variable-Models-and-the-EM-Algorithm">&#182;</a></h3>
</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<h3 id="Part-III---Scalable-Methods-for-Bayesian-Learning">Part III - Scalable Methods for Bayesian Learning<a class="anchor-link" href="#Part-III---Scalable-Methods-for-Bayesian-Learning">&#182;</a></h3><ul>
<li>Probabilistic PCA</li>
<li>Variational Autoencoders</li>
<li>The Reparametrization Trick</li>
<li>Generative Models and Adverserial Learning</li>
</ul>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<h3 id="Part-IV---Gaussian-Processes-and-Optimization">Part IV - Gaussian Processes and Optimization<a class="anchor-link" href="#Part-IV---Gaussian-Processes-and-Optimization">&#182;</a></h3><ul>
<li>Gaussian Processes</li>
<li>Bayesian Optimization</li>
<li>MCMC</li>
</ul>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<h3 id="Part-V---Bayesian-Deep-Learning">Part V - Bayesian Deep Learning<a class="anchor-link" href="#Part-V---Bayesian-Deep-Learning">&#182;</a></h3><ul>
<li>Bayesian Neural Networks</li>
<li>Variational Dropout</li>
<li>Sparse Bayesian Networks</li>
</ul>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<h1 id="Part-I---Recap-of-Bayesian-ML">Part I - Recap of Bayesian ML<a class="anchor-link" href="#Part-I---Recap-of-Bayesian-ML">&#182;</a></h1>
</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Bayes'-Theorem">Bayes' Theorem<a class="anchor-link" href="#Bayes'-Theorem">&#182;</a></h1>$$p(y|x) = \frac{p(x|y)p(y)}{p(x)} = \frac{p(x|y)p(y)}{\int{p(x|y)p(y) dy}}$$<p>or in words</p>
$$Posterior = \frac{Likelihood \cdot Prior}{Evidence}$$
</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="The-Inference-Problem">The Inference Problem<a class="anchor-link" href="#The-Inference-Problem">&#182;</a></h1><h4 id="Problem">Problem<a class="anchor-link" href="#Problem">&#182;</a></h4><p>Given data $X = (x_1,\ldots,x_n)$ (iid) from $p(x|\theta)$, infere $\theta$.</p>
<h4 id="MLE-(Maximum-Likelihood-Estimation)">MLE (Maximum Likelihood Estimation)<a class="anchor-link" href="#MLE-(Maximum-Likelihood-Estimation)">&#182;</a></h4>$$\theta_{ML} = \arg \max \prod_{i=1}^n{p(x_i|\theta)} = \arg \max \sum_{i=1}^n{\log p(x_i|\theta)}$$
</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="The-Inference-Problem">The Inference Problem<a class="anchor-link" href="#The-Inference-Problem">&#182;</a></h1><h4 id="Problem">Problem<a class="anchor-link" href="#Problem">&#182;</a></h4><p>Given data $X = (x_1,\ldots,x_n)$ (iid) from $p(x|\theta)$, infere $\theta$.</p>
<h4 id="Bayesian-Inference">Bayesian Inference<a class="anchor-link" href="#Bayesian-Inference">&#182;</a></h4><p>Encode uncertainty about $\theta$ in terms of a <strong>prior distribution $p(\theta)$</strong> and apply Bayes' theorem:</p>
$$p(\theta|X) = \frac{\prod_{i=1}^n p(x_i|\theta)p(\theta)}{\int \prod_{i=1}^n p(x_i|\theta)p(\theta) d\theta}$$
</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="The-Inference-Problem">The Inference Problem<a class="anchor-link" href="#The-Inference-Problem">&#182;</a></h1><h4 id="Bayesian-Inference">Bayesian Inference<a class="anchor-link" href="#Bayesian-Inference">&#182;</a></h4>$$p(\theta|X) = \frac{\prod_{i=1}^n p(x_i|\theta)p(\theta)}{\int \prod_{i=1}^n p(x_i|\theta)p(\theta) d\theta}$$<p>Bayesian Inference provides a full <em>posterior</em> distribution over $\theta$. <strong>Computing the mode is called "Poor Man's Bayes" and gives the MAP (Maximum A Posteriori) estimate.</strong></p>
<p>Note that the evidence does not depend on $\theta$ and therefore is irrelevant for computing the MAP estimate: $$\theta_{MAP} = \arg \max p(x|\theta)p(\theta) = \arg \max \{\log p(x|\theta) + \log p(\theta) \}$$</p>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="The-Inference-Problem">The Inference Problem<a class="anchor-link" href="#The-Inference-Problem">&#182;</a></h1><h4 id="Bayesian-Inference">Bayesian Inference<a class="anchor-link" href="#Bayesian-Inference">&#182;</a></h4><p>Note that the evidence does not depend on $\theta$ and therefore is irrelevant for computing the MAP estimate: $$\theta_{MAP} = \arg \max p(x|\theta)p(\theta) = \arg \max \{\log p(x|\theta) + \log p(\theta) \}$$</p>
<p><strong>Note further that, while the likelihood depends exponentially on $n$, the prior remains constant. Hence, eventually, the data will <em>overwhelm the prior</em> and the MAP will converge towards the MLE.</strong> Likewise, the posterior will become peaked around the MAP estimate.</p>
<p>It is often the case, that when $n$ grows, frequentist and Bayesian perspective will yield the same result! They do not contradict each other. You should have both tools in your toolbox.</p>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Note-on-MAP-estimation">Note on MAP estimation<a class="anchor-link" href="#Note-on-MAP-estimation">&#182;</a></h1><p>The MAP is a popular point estimate because calculating the mode (other than the mean or median for example) is an <strong>optimization problem</strong>, for which we have many efficient algorithms available. However, the are <strong>drawbacks</strong> and I want to point out a few as a reference:</p>
<ul>
<li>Mean or median often summarize the posterior distribution better than the mode.</li>
<li>Point estimates lack a measure of uncertainty.</li>
<li>This can lead to overfitting.</li>
<li>The MAP estimate depends on the parametrization of the distribution: Let $y=f(x)$, then $\bar x = \arg \max p_x(x)$ and $\bar y = \arg \max p_{y}(y)$ does not imply $\bar y = f(\bar x)$.</li>
</ul>
<p>(See, for example, K.P. Murphy: Machine Learning, ch. 5.2.1)</p>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Generative-vs-Discriminative-Models">Generative vs Discriminative Models<a class="anchor-link" href="#Generative-vs-Discriminative-Models">&#182;</a></h1><h4 id="Generative-Models">Generative Models<a class="anchor-link" href="#Generative-Models">&#182;</a></h4><p>Model the joint distribution $p(x,y,\theta) = p(x,y|\theta)p(\theta)$. This means that once we have a trained model we can generate new data $(x,y)$.</p>
<p>Examples are <em>Naive Bayes</em> or <em>Generative Adversarial Networks (GANs)</em>.</p>
<p>Generative models can be difficult to train, because the feature space $X$ is usually much more complicated than the label space $Y$, e.g.</p>
<ul>
<li>In image classification, the space of all images $X$ is much more complicated than the $Y$ space of image classes.</li>
<li>However, there are other examples. In machine translation $X$ and $Y$ space are equal.</li>
</ul>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Generative-vs-Discriminative-Models">Generative vs Discriminative Models<a class="anchor-link" href="#Generative-vs-Discriminative-Models">&#182;</a></h1><h4 id="Discriminative-Models">Discriminative Models<a class="anchor-link" href="#Discriminative-Models">&#182;</a></h4><p>If we directly model the class posterior, e.g. $p(y=c, \theta |x)$, we do not need to know about the distribution of the observed variables. This is called a discriminant classifier. We usually assume that the prior over the model parameters $\theta$ does not depend on $X$, thus</p>
$$p(y,\theta | x) = p(y | x,\theta)p(\theta)$$<p>A basic example is <em>Logistic Regression</em>.</p>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Generative-vs-Discriminative-Models">Generative vs Discriminative Models<a class="anchor-link" href="#Generative-vs-Discriminative-Models">&#182;</a></h1><p>Both have advantages and disadvantages! Again, have both in your toolbox.</p>
<p>Generative Classifiers impose more strict assumptions on the model. If the assumptions are correct, generatives models can be trained <strong>faster and with less data</strong>. But, joint distributions are harder to estimate and lead to uncertainty, therefore <strong>discriminative models are often more accurate</strong>.</p>
<p>Models like Naive Bayes can be trained extremely easily by simply counting. However, the strong assumptions are often not met and lead to ill-calibrated probability outputs.</p>
<p>Generative models, however, have the advantage to be able to <strong>deal with missing data</strong> and can be extended more easily to semi-supervised learning. (See, for example, B. Marlin (2008): Missing Data Problems in Machine Learning)</p>
<p>Discriminative models, since they do not care about the distribution of the data, have the advantage that they allow arbitrary <strong>feature preprocessing</strong>.</p>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Bayesian-Statistics">Features of Bayesian Statistics<a class="anchor-link" href="#Features-of-Bayesian-Statistics">&#182;</a></h1><h4 id="Sequential-Learning">Sequential Learning<a class="anchor-link" href="#Sequential-Learning">&#182;</a></h4><p>Bayesian inference can be done sequentially or in batches which makes it a great candidate for <strong>online or minibatch learning</strong>, the preferred method when dealing with large datasets.</p>
<ul>
<li>Define Prior</li>
</ul>
<div class="highlight"><pre><span></span>Posterior <span class="o">&lt;-</span> Likelihood x Prior
<span class="p">(</span>New<span class="p">)</span> Prior <span class="o">&lt;-</span> Posterior
</pre></div>
<ul>
<li>Observe new data</li>
</ul>
<div class="highlight"><pre><span></span>Update Likelihood
<span class="p">(</span>New<span class="p">)</span> Posterior <span class="o">&lt;-</span> Likelihood x <span class="p">(</span>New<span class="p">)</span> Prior
</pre></div>
<ul>
<li>Repeat</li>
</ul>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Bayesian-Statistics">Features of Bayesian Statistics<a class="anchor-link" href="#Features-of-Bayesian-Statistics">&#182;</a></h1><h4 id="Bayesian-Inference-as-Ensemble-Learning">Bayesian Inference as Ensemble Learning<a class="anchor-link" href="#Bayesian-Inference-as-Ensemble-Learning">&#182;</a></h4><p>Suppose we work with training data $(X_{tr}, Y_{tr})$ and a discriminator $p(Y,\theta | X)$.</p>

</div>
</div>
</div><div class="fragment">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Training the model consists of calculating</p>
$$p(\theta | X_{tr}, Y_{tr}) = \frac{p(Y_{tr}|X_{tr},\theta)p(\theta)}{\int p(Y_{tr}|X_{tr},\theta)p(\theta)d\theta}$$<p>This is an <strong>ensemble</strong> of models over the distribution of $\theta$.</p>

</div>
</div>
</div></div><div class="fragment">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>At test time, we perform <strong>model averageing</strong> via the posterior over $\theta$:</p>
$$p(y_{test}|x_{test},X_{tr},Y_{tr}) = \int p(y_{test}|x_{test},\theta)p(\theta|X_{tr},Y_{tr})d\theta$$
</div>
</div>
</div></div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Bayesian-Statistics">Features of Bayesian Statistics<a class="anchor-link" href="#Features-of-Bayesian-Statistics">&#182;</a></h1><h4 id="Intractability">Intractability<a class="anchor-link" href="#Intractability">&#182;</a></h4><p>Model averageing outperforms a single model in the $\theta$ family (e.g. the MAP model $\arg \max_{\theta} p(\theta|X,Y)$ or a model found through cross-validation) since the full posterior contains all information about $X$ and $Y$ that the model can detect.</p>
<p>However, the <strong>integrals</strong> that appear during training and test are usually <strong>intractable</strong>!</p>

</div>
</div>
</div><div class="fragment">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There is one family of exception: conjugate priors! We will talk more about these in the next segment (to the right).</p>
<p>For now, remember that distributions $p(y)$ and $p(x|y)$ are <strong>conjugate</strong> iff $p(y|x)$ belongs to the same (parametric) family as $p(y)$. If so, the formulas above will have <strong>closed-form solutions</strong>!</p>

</div>
</div>
</div></div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Bayesian-Statistics">Features of Bayesian Statistics<a class="anchor-link" href="#Features-of-Bayesian-Statistics">&#182;</a></h1><h4 id="Intractability">Intractability<a class="anchor-link" href="#Intractability">&#182;</a></h4><p>For models where this is intractable, approximate methods exist based on sampling methods like MCMC or variational apprxoimations.</p>
<p>Variational inference is based on the idea to pick a tractable family of approximations to the posterior and then pick the one that is closest to the true posterior, e.g. by minimizing the Kulback-Leibler distance between the two.</p>
<p>We will discuss these methods and scalable versions of them in following chapters.</p>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Bayesian-Statistics">Features of Bayesian Statistics<a class="anchor-link" href="#Features-of-Bayesian-Statistics">&#182;</a></h1><h4 id="Model-Selection,-Overfitting-and-Regularization">Model Selection, Overfitting and Regularization<a class="anchor-link" href="#Model-Selection,-Overfitting-and-Regularization">&#182;</a></h4><p>As mentioned, for large datasets MAP estimates and full Bayesian posteriors lead to similar results, but they can be very different for small or ambiguous datasets. In such a case, the posterior will be vague, which leads to a very broad (or uninformative) predictive distribution. However, just using the MAP model (or <em>plug-in aproximation</em>) is simple, but can underestimate the uncertainty. The Bayesian approach has the nice property that it <strong>starts out broad</strong> and becomes narrower as we become more certain or see more data. This coincides better with our intuitive understanding. Starting out broadly can prevent overfitting!</p>
<p>The Bayesian approach also has the advantage that it can <strong>incorporate regularization</strong> within the Bayesian framework by prescriping appropriate priors, e.g.</p>
<ul>
<li>Lasso and Ridge Regression, for example, can be seen as specially cases with Laplace and Gaussian priors.</li>
<li>In models based on count values, like Naive Bayes for word counts, priors can naturally prevent Black-Swan type paradoxes when the model encounters a previously unseen word (which can otherwise lead to zero probabilities)</li>
</ul>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Bayesian-Statistics">Features of Bayesian Statistics<a class="anchor-link" href="#Features-of-Bayesian-Statistics">&#182;</a></h1><h4 id="Scalability">Scalability<a class="anchor-link" href="#Scalability">&#182;</a></h4><p>Scalability has traditionally been an issue for Bayesian methods. However, tools have emerged that make variational and MCMC inference scalable.</p>
<p>We will present some of these in the following chapters.</p>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Priors">Priors<a class="anchor-link" href="#Priors">&#182;</a></h1><p>One hotly debated issue of Bayesian statistics is the use of priors. For Bayesians this is necessary because nobody operates in a vacuum. However, there are some ways to reduce the impact of one's prior assumptions, e.g. using <strong>uninformative priors</strong>.</p>
<p><strong>Conjugate priors</strong> simplify computations and are often easy to interpret. As a reference, we provide the standard example of the <strong>beta-binomial model</strong>, which models the <strong>probability to see heads in a series of coin flips</strong>. Let $X_i \sim Ber(\theta), \theta \in [0,1]$</p>

</div>
</div>
</div><div class="fragment">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li><strong>Likelihood</strong>: $p(X|\theta) = \theta^{N_{head}}(1-\theta)^{N_{tail}}$, with $N_{head} \sim Bin(N_{head}+N_{tail}, \theta)$ (the binomial distribution)</li>
</ul>

</div>
</div>
</div></div><div class="fragment">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The <strong>conjugate prior</strong> of the Bernoulli distribution is the Beta distribution<ul>
<li>$p(\theta) \propto \theta^{a-1}(1-\theta)^{b-1} \propto Beta(\theta|a,b)$, note that $a$ and $b$ are called hyper-parameters</li>
<li>If we have no prior knowledge of $\theta$ we can choose the uniform distribution which is given via $Beta(\theta|1,1)$ (see plot below). This is a way to implement an <strong>uninformative prior</strong> (more later).</li>
</ul>
</li>
</ul>

</div>
</div>
</div></div></section><section>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="k">import</span> <span class="n">beta</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">beta</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
       <span class="s1">&#39;r-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;beta a=1 b=1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
       <span class="s1">&#39;b-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;beta a=2 b=2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span>
       <span class="s1">&#39;g-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;beta a=1 b=2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
       <span class="s1">&#39;y-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;beta a=3 b=1&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt output_prompt">Out[23]:</div>



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.legend.Legend at 0x20b755f6f28&gt;</pre>
</div>

</div>

<div class="output_area">
<div class="prompt"></div>



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Priors">Priors<a class="anchor-link" href="#Priors">&#182;</a></h1><ul>
<li>Likelihood: $p(X|\theta) = \theta^{N_{head}}(1-\theta)^{N_{tail}}$, with $N_{head} \sim Bin(N_{head}+N_{tail}, \theta)$</li>
<li><p>Conjugate prior: $p(\theta) \propto Beta(\theta|a,b)$</p>
</li>
<li><p><strong>Posterior</strong>: Multiplying likelihood and prior amounts to adding the exponents and yields $\propto Beta(\theta|N_{head}+a, N_{tail}+b)$</p>
</li>
</ul>

</div>
</div>
</div><div class="fragment">
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The hyper-parameters of the prior, $a$ and $b$, are therefore called <strong>pseduo-counts</strong>. The total strength of the prior is determined by the sum of pseduo-heads and pseudo-tails $a+b$.</p>
<p>We can see, again, how Bayesian inference works well sequentially. Updating the posterior is simply adding the new counts of heads and tails to the prior counts.</p>
<p>Note that priors need not necessarily be <strong>proper</strong> distributions (i.e. integrate to 1). This is possible as long as the posterior is proper!</p>

</div>
</div>
</div></div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Priors">Priors<a class="anchor-link" href="#Priors">&#182;</a></h1><h4 id="Jeffreys-prior">Jeffreys prior<a class="anchor-link" href="#Jeffreys-prior">&#182;</a></h4><p><strong>Jeffreys priors</strong> are a general purpose technique to design <strong>uninformative</strong> priors. The key insight behind Jeffreys priors is that uninformative priors should be invariant under reparametrization. One can show that this can be achieved when the prior is proportional to the square root of the determinant of the <strong>Fisher information</strong>:</p>
$$ p(\theta) \propto \sqrt{\det \mathcal{I}(\theta)} $$<p>Note that the Fisher information is not the only choice with this property. Also, remember that Jeffreys priors can be improper. For example, the Jeffreys prior for the Gaussian mean parameter $\mu$ (with fixed standard deviation) is $1$ and independent of $\mu$.</p>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Priors">Priors<a class="anchor-link" href="#Priors">&#182;</a></h1><h4 id="Jeffreys-prior">Jeffreys prior<a class="anchor-link" href="#Jeffreys-prior">&#182;</a></h4><p>Let's revisit the example of the beta-binomial model where we previously suggested a uniform, or $Beta(1,1)$, distribution over $[0,1]$ as an uninformative prior.</p>
<p>We will show that the Jeffreys prior for $X \sim Ber(\theta)$ is actually $Beta(\frac{1}{2},\frac{1}{2})$:</p>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Priors">Priors<a class="anchor-link" href="#Priors">&#182;</a></h1><h4 id="Jeffreys-prior">Jeffreys prior<a class="anchor-link" href="#Jeffreys-prior">&#182;</a></h4><p>The log-likelihood is $p(X|\theta) = X\log\theta + (1-X)\log(1-\theta)$ with derivative $\frac{X}{\theta}-\frac{1-X}{1-\theta}$.
The second derivative is $J(\theta) = -\frac{X}{\theta^2}-\frac{1-X}{(1-\theta)^2}$.</p>
<p>The Fisher information is then</p>
$$\mathcal{I}(\theta) = \mathbb{E} \left[ -J(\theta|X) \mid X\sim\theta\right] = \frac{\theta}{\theta^2} + \frac{1-\theta}{(1-\theta)^2} = \frac{1}{\theta(1-\theta)}$$<p>and, hence, Jeffreys prior is</p>
$$p(\theta) \propto \frac{1}{\sqrt{\theta(1-\theta)}} \propto Beta(\frac{1}{2},\frac{1}{2})$$
</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Priors">Priors<a class="anchor-link" href="#Priors">&#182;</a></h1><h4 id="Robust-Priors-and-Mixtures-of-Conjugate-Priors">Robust Priors and Mixtures of Conjugate Priors<a class="anchor-link" href="#Robust-Priors-and-Mixtures-of-Conjugate-Priors">&#182;</a></h4><p>If we have prior assumptions but are not very confident in them, it makes sense to use <strong>robust priors</strong> which have heavy tails and are less peaked around the prior mean! Since they can be computationally disadvantageous, one can resort to <strong>mixtures of conjugate priors</strong> (which are again conjugate) by introducing a latent mixture variable to approximate a desired distribution.</p>
<p>In our beta-binomial model, for example, we could introduce a prior</p>
$$p(\theta) = \frac{1}{2} \cdot Beta(\theta|10,10) + \frac{1}{2} \cdot Beta(\theta|20,10)$$<p>This would allow us to model a situtation where we believe the coin is either fair <em>or</em> biased towards heads! It is easy to show that the posterior would also be a mixture of $Beta$ dsitributions.</p>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Frequentist-Statistics">Features of Frequentist Statistics<a class="anchor-link" href="#Features-of-Frequentist-Statistics">&#182;</a></h1><p>Frequentist statistics <strong>avoids priors and using the Bayes rule</strong> by using sampling distributions instead of treating parameters as random variables. The <strong>sampling distribution</strong> is the distribution of an estimator when applied to multiple datasets sampled from the true (but unknown) distribution.</p>
<p>Frequentist statistics is the <strong>dominant paradigm used in machine learning</strong>.</p>
<p>We will discuss a few basic concepts of frequentist statistics for reasons of comparison.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Frequentist-Statistics">Features of Frequentist Statistics<a class="anchor-link" href="#Features-of-Frequentist-Statistics">&#182;</a></h1><h4 id="The-Sampling-Distribution">The Sampling Distribution<a class="anchor-link" href="#The-Sampling-Distribution">&#182;</a></h4><p>To compute a parameter estimate $\hat \theta$, we apply an estimator $\Gamma$ to some dataset $X$: $\hat \theta = \Gamma(X)$. $\theta$ is fixed. $X$ is random. (In the Bayesian framework $\theta$ is random and the data is fixed.) Uncertainty is measured by the sampling distribution of the estimator.</p>
<p>One way to approximate the sampling distribution from $X$ is the <strong>bootstrap</strong> which repeatedly samples (either with replacement or parametric using $\hat \theta$) from the dataset $X$.</p>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Frequentist-Statistics">Features of Frequentist Statistics<a class="anchor-link" href="#Features-of-Frequentist-Statistics">&#182;</a></h1><h4 id="Model-Selection">Model Selection<a class="anchor-link" href="#Model-Selection">&#182;</a></h4><p>Again, in frequentist theory the parameter is fixed and the data is random and in Bayesian theory the parameter is random, Thus, frequentist decision theory relies on $p(X|\theta^*)$ ($\theta^*$ being the "true" parameter) while Bayesian theory relies on $p(\theta|X,\epsilon)$. Note that $\theta^*$ is unkown. This is not an issue in the Bayesian approach where we average over the unkown parameter and condition on the data.</p>
<p>Frequentist statistics can therefore not directly select the best estimator for an unkown parameter $\theta$. It circumvents this problem by looking at a situation where observable quantaties (think input features to a supervised classification problem) and their true responses (think corresponding labels) are available.</p>
<p>This gives rise to the framework of <strong>Empircal Risk Minimization</strong>. The risk can be upper-bounded using <strong>Statistical Learning Theory</strong> (e.g. using <strong>Vapnik-Chervonenkis</strong> dimension etc.). Estimators are chosen using <strong>cross-validation (CV)</strong>. I assume familiarity with these topics and do not provide further details, but want to emphasize the different perspectives.</p>

</div>
</div>
</div></section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Features-of-Frequentist-Statistics">Features of Frequentist Statistics<a class="anchor-link" href="#Features-of-Frequentist-Statistics">&#182;</a></h1><p>Frequentist statistics has many flaws. Mainly that when computing likelihoods, it relies on some future, hypothetical distribution instead of only observable data as in the case of Bayesian statistics.</p>
<p>This somewhat unintitive approach is, however, very successful for a reason. It is often computationally more efficient and flexible. Also, remember that frequentist and Bayesian statistics lead to similar or the same results in a wide variety of scenarios!</p>

</div>
</div>
</div></section></section><section><section>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>© Olaf Wied 2018</p>
<p>Part I - Recap of Bayesian ML</p>
<h1 id="Further-reading">Further reading<a class="anchor-link" href="#Further-reading">&#182;</a></h1><p>The previous slides only provide a brief summary of some(!) of the basic concepts and terminology encountered in Bayesian Statistics. If you are unfamiliar with many of these concepts, I recommend a look at "Bayesian Data Analysis" by Gelman et al. and Murphy's "Machine Learning (A Probabilistic Perspective)" before moving on. A lot of the previous slides follow their presentation.</p>

</div>
</div>
</div></section></section>
</div>
</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
