{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this final project, you will implement Latent Dirichlet Allocation (LDA) for unsupervised topic discovery in text analysis. You will derive the necessary algebra and implement LDA with variational inference from first principles bringing together much of the content from *Part II* of the course.\n",
    "\n",
    "## The LDA model\n",
    "\n",
    "As you probably know, LDA is a popular tool for topic modelling. LDA makes the following assumptions:\n",
    "\n",
    "- every document is represented as a bag of words (independent of their order)\n",
    "- every document is a (discrete) mixture of topics\n",
    "- every topic is a distribution over the set of all words\n",
    "\n",
    "The fully specified probabilistic model is as follows:\n",
    "\n",
    "Assume $M$ documents, $K$ topics, and $N$ words in a document and $V$ the size of the vocabulary. We will follow for the most part common notation from the paper [\"Online Learning for Latent Dirichlet Allocation\"](https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf) but note that you will likely encounter other variable names (e.g. in Murphy's ML book).\n",
    "\n",
    "Let $\\theta$ be the topic probabilities and $\\alpha$ the topic mixture variables. For each document we draw a vector of topic probabilities:\n",
    "\n",
    "$$\\theta_d|\\alpha \\sim Dir(\\alpha) \\propto \\prod_{k=1}^K\\theta_{dk}^{\\alpha_k-1}$$\n",
    "\n",
    "Let $\\eta$ be the word mixture variable for the topics. For each topic $k$ we also draw a vector of word probabilities $\\beta_k$:\n",
    "\n",
    "$$\\beta_k|\\eta \\sim Dir(\\eta) \\propto \\prod_{v=1}^V\\phi_{kv}^{\\eta_v-1} $$\n",
    "\n",
    "\n",
    "Each word is assigned its own topic. For each word $i$ in document $d$ we draw a topic index\n",
    "\n",
    "$$ z_{di}|\\theta_d \\sim Cat(\\theta_d) = \\prod_{k=1}^K\\theta_{k}^{[z_{di}=k]} \\Leftrightarrow p(z_{di}|\\theta_d) = \\theta_{dz_{di}}\\\\ z_{di}\\in\\{{1,\\ldots,K}\\}$$\n",
    "\n",
    "and the word $w_{di}$\n",
    "\n",
    "$$w_{di}|z_{di} \\sim Cat(\\beta_k) = \\prod_{k=1}^K \\prod_{v=1}^V \\beta_{t,v}^{[w_{di}=v][z_{di}=k]} \\Leftrightarrow p(w_{di}|z_{di}) = \\beta_{z_{di}w_{di}}\\\\ \n",
    "w_{di} \\in \\{1,\\ldots,V\\} \\\\\n",
    "\\text{with } d=1,\\ldots,M \\text{ and } i=1,\\ldots,N$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Plate notation this looks like follows (source https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)\n",
    "\n",
    "![Plate Notation for LDA (source Wikipedia)](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)\n",
    "\n",
    "This hierarchical Bayesian model has three levels (documents, topics and words)!\n",
    "\n",
    "Thus, it follows that \n",
    "\n",
    "$$p(w_{di}=w|\\theta_d,\\beta) = \\sum_{k=1}^K p(w_{di}=w|z_{di}=k)p(z_{di}=k|\\theta_d) = \\sum_{k=1}^K\\theta_{dk}\\beta_{kw}$$\n",
    "\n",
    "which states that we can decompose the matrix of word counts into factors of topic weights $\\theta$ and a dictionary of topics $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter dimensions\n",
    "$\\theta = D \\times K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Bayes\n",
    "We use the mean-field approximation by factorizing the variational posterior $q(Z,\\theta,\\beta)$ into its parameterized parts $q(z_{di}=k) = \\phi_{dw_{di}k}$, $q(\\theta_d)=Dir(\\gamma_d)$ and $q(\\beta_k) = Dir(\\lambda_k)$. Notice that $\\lambda$ defines the model topics.\n",
    "\n",
    "Variational inference maximizes the ELBO:\n",
    "\n",
    "$$\\mathcal{L}(W,\\phi,\\gamma,\\lambda) = \\mathbb{E}_q\\left[ \\log p(W,Z,\\theta,\\beta|\\alpha,\\eta) - \\log q(Z,\\theta,\\beta) \\right]$$\n",
    "\n",
    "Recall that according to the process defined above, we can generate a document $d$ as follows:\n",
    "\n",
    "$$p(w_d,z_d,\\theta_d, \\beta | \\alpha, \\eta) = \\underbrace{p(\\theta_d|\\alpha)}_{\\text{topic probs }} \\underbrace{p(\\beta|\\eta)}_{\\text{ topic mixtures}\\\\ \\text{ over vocab }} \\underbrace{\\prod_{i=1}^N}_{\\text{ for every word}\\\\ \\text{ in document }} \\underbrace{p(z_{di}|\\theta_d)p(w_{di}|z_{di},\\theta_d,\\beta)}_{\\text{ generate topic}\\\\ \\text{ and draw word}}$$\n",
    "\n",
    "and that our corpus is an collection of independent documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the ELBO expands to\n",
    "\n",
    "$$\\mathcal{L}(W,\\phi,\\gamma,\\lambda) \\\\\n",
    "= \\sum_{d=1}^M \\mathbb{E}_q \\left(\\log p(\\theta_d|\\alpha) + \\frac{1}{M}\\log p(\\beta|\\eta) + \\log p(z_d|\\theta_d)+\\log p(w_d|z_d,\\beta,\\theta_d) - \\log q(z_d) - \\log q(\\theta_d) - \\frac{1}{M}\\log q(\\beta)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$=\\sum_{d=1}^M \\mathbb{E}_q \\left( \\sum_{k=1}^K \\left((\\alpha_k-1)\\log \\theta_{dk}\\right) + \\frac{1}{M}\\sum_{k=1}^K\\left(\\sum_{v=1}^V(\\eta_v-1)\\log\\phi_{kv}\\right) + \\sum_{i=1}^N \\log \\theta_{z_{di}}  + \\sum_{i=1}^N \\log\\beta_{z_{di}w_{di}} - \\sum_{k=1}^K\\left(\\sum_{i=1}^N\\log\\phi_{dw_{di}k}\\right) - \\sum_{k=1}^K \\left((\\gamma_k-1)\\log \\theta_{dk}\\right) -\\frac{1}{M}\\sum_{k=1}^K\\left(\\sum_{v=1}^V (\\gamma_v-1)\\log \\lambda_{dv}\\right) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "for pgk in [tf,np]:\n",
    "    print(pkg,pkg.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
